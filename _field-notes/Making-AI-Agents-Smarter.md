---
title: "ðŸ¤– Making AI Agents Smarter: How RAG and Vector Databases Power Custom Intelligence"
date: 2024-07-22
last_modified_at: 2025-08-16
excerpt: "To build autonomous agents that can reason, retrieve, and act, prompts alone won't take you far. You need the logic behind the prompts. Dive deep into how agentic workflows work, how data is broken down and fed to the brain of AI, and why RAG is changing the game."
header:
  teaser: /assets/images/rag-vector-databases.jpg
  overlay_image: /assets/images/rag-vector-databases.jpg
  overlay_filter: 0.5
  caption: "Photo credit: [Sanuwar Rashid's original article]"
categories:
  - field-notes
  - ai-engineering
tags:
  - RAG
  - Vector-Databases
  - AI-Agents
  - Embeddings
  - LLM
  - Agentic-AI
  - MLOps
toc: true
toc_label: "Contents"
toc_icon: "database"
author_profile: true
original_author: "Sanuwar Rashid"
original_url: "https://medium.com/@sanuwar.rashid"
---

*Originally published by [Sanuwar Rashid]({{ page.original_url }}) on {{ page.date | date: "%B %d, %Y" }}*
{: .notice--info}

## Beyond Prompts: The Logic Behind Intelligent Agents

To build autonomous agents that can reason, retrieve, and act â€” especially using low-code tools like n8n, Flowise, or LangFlow â€” prompts alone won't take you far. You need the logic behind the prompts. This exploration goes deeper into how agentic workflows work, how data is broken down and fed to the brain of AI, and why RAG (Retrieval-Augmented Generation) is changing the game.

## Three Ways to Give LLMs Additional Knowledge

We often want to give additional knowledge to large language models (LLMs). There are a few ways to do that:

1. **In-context learning** â€“ feeding information directly into the prompt window
2. **RAG pipelines** â€“ retrieving relevant information from external sources like vector databases
3. **Fine-tuning** â€“ retraining the model with new data, which is expensive and rigid

## The Context Window Problem

Let's say you want to teach an LLM something new. You can pass context (like a document) into the prompt, and the LLM might "learn" it momentarily. But context window has limits. If you try to jam in a thousand PDFs, you'll hit the ceiling â€” the token limit. And when you overload the context window, the LLM might not even follow what you're trying to say. Sounds like drifting? Yes, exactly.

What is a context window, by the way? It's the amount of "space" the model has to remember things while answering your query. Too much information, and it starts forgetting the beginning before it reaches the end.

## RAG: The Smart Assistant Approach

In-context learning is like showing an LLM a sticky note just before it answers a question â€” it can read what's on the note, but once the space runs out, it can't remember more. Retrieval-Augmented Generation (RAG), on the other hand, is like giving the model a smart assistant who can flip through a library of sticky notes and hand it only the most relevant one each time you ask something. Instead of overloading the model with all your data upfront, RAG dynamically fetches just the pieces that matter, right when they're needed.

Ever tried a Custom GPT inside ChatGPT? You can upload your resume, and the GPT remembers it across sessions. You can ask it to analyze job postings based on your profile. This persistent memory works not because the GPT memorized you â€” but because it retrieves the right context from the uploaded document every time you ask.

If you've ever used a Custom GPT that remembers your uploaded resume or project notes across chats â€” you've used RAG. It doesn't memorize your files; it fetches relevant content every time you ask.

## RAG vs Fine-Tuning: The Key Differences

RAG is very different from fine-tuning, which actually rewires the model's internal weights to "bake in" new knowledge permanently. Fine-tuning is slower, more expensive, and requires retraining the model whenever your knowledge base changes. RAG is faster, cheaper, and more flexible â€” it acts like a just-in-time memory layer, not a brain transplant.

![In-Context Learning vs RAG vs Fine-Tuning Comparison](/assets/images/in-context-vs-rag-vs-finetuning.jpg)
*A 3-column comparison showing In-Context Learning ("All-at-once context," limited by token window), RAG ("Retrieve as needed," pulls relevant docs per query), and Fine-Tuning ("Train for specifics," resource-intensive but tailored).*
{: .text-center}

## How RAG Works Behind the Scenes

Here's how that happens behind the scene:

An embedding model takes your documents and converts them into numerical representations â€” vectors. These vectors are not stored on a literal 3D chessboard, but in a high-dimensional space (think 768D or 1536D), where similar meanings end up closer together.

These vectors are stored in a vector database. When you ask a question that the LLM doesn't know, a retriever fetches similar chunks from the vector database based on your query. The result? Relevant pieces of text are fed back into the LLM's context window so it can answer your question like it knew the answer all along.

![Vector Database and Similarity Search Process](/assets/images/vector-database-similarity-search.jpg)
*A diagram showing how source data is embedded into vectors, indexed, stored in a vector database, and retrieved based on similarity scoring to return top-k relevant results to the application.*
{: .text-center}

## The Chunking and Retrieval Process

Documents are broken into chunks. Each chunk is usually a few hundred tokens (words and characters). Say your chunk size is 500 â€” that means each chunk has up to 500 tokens. When a query comes in, the system retrieves Top-K chunks â€” say, the top 4 most relevant chunks based on similarity. If Top-K = 4 and chunk size = 500, you're feeding about 2000 tokens of relevant data back into the LLM.

So, when the user asks a question â†’ the retriever does a similarity search â†’ finds top K relevant chunks â†’ sends those chunks into the LLM â†’ the LLM answers based on that injected context.

![Complete RAG Pipeline Architecture](/assets/images/complete-rag-pipeline.jpg)
*This diagram breaks down a complete RAG pipeline: from setting up a knowledge base to generating final answers with citations. It showcases how vector search, embeddings, and large language models (LLMs) come together to deliver context-aware, efficient responses â€” a foundation for many GenAI applications today.*
{: .text-center}

## The Complete RAG Pipeline

The possibility of RAG starts here! And with smart orchestration, agents can do even more â€” retrieving knowledge, making decisions, and even executing actions based on retrieved context.

## Key Takeaways

In sum, the magic isn't in just feeding data. It's in embedding, chunking, retrieving, and injecting the right information at the right time. That's how we move from raw text to smart actions â€” one chunk at a time.

The RAG approach enables:
- **Dynamic knowledge access** without token limits
- **Cost-effective updates** to knowledge bases
- **Contextual relevance** through similarity search
- **Scalable intelligence** for autonomous agents

---

**About the Original Author**: Sanuwar Rashid is a Senior Engineer at Humana, specializing in MLOps to Agentic AI and automating intelligence at scale.
{: .notice--primary}

### Related Topics
{: .no_toc}

This article connects to broader themes in AI engineering, including vector search optimization, embedding model selection, and agentic workflow design.